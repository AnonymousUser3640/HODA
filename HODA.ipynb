{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HODA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuicbSJojHPJ"
      },
      "source": [
        "**HODA: Hardness-Oriented Detection of Model Extraction Attacks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhAcpZILYvZf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alTCzFQqdgy4"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "if device == 'cuda':\n",
        "  print(torch.cuda.get_device_name(0))\n",
        "\n",
        "%cd /content/drive/MyDrive/HODA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ2EkEEWdlFy"
      },
      "source": [
        "# Target Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpT2UKk_dbga"
      },
      "source": [
        "##Config target model and datasets directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiXr-lKrddEZ"
      },
      "source": [
        "max_hardness_degree = 10            # valid values: 10 (hardness degree is calculated by 11 subclassifiers) and 99 (hardness degree is calculated by all 100 subclassifirs) - Since HODA uses 11 subclassifiers, use 10 to evaluate HODA.\n",
        "\n",
        "# Traget model confifuration\n",
        "target_model_dataset = 'cifar10'   # valid values: cifar10, cifar100, caltech256, and cub\n",
        "target_model_arch = 'resnet18'      # valid values: resnet18, densenet121, and mobilenet\n",
        "target_model_training = 'load'      # valid values: train (train a new target model) and load (load existing target model) - Since CIFAR10 (ResNet18) and CIFAR100 (ResNet18) target models exists, use 'load' option.\n",
        "\n",
        "# Directory configuration\n",
        "local_dir = '/content/data/'\n",
        "ds_dir = '/content/drive/MyDrive/HODA/dataset/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8WNYh4eWKAc"
      },
      "source": [
        "## Target model dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjBn9nVfdlZe"
      },
      "source": [
        "from hoda_dataset import dataset_class\n",
        "target_ds = dataset_class(target_model_dataset, clean = True, valid = True, shuffle = True, aug = True, local_directory= local_dir, dataset_drive_directory= ds_dir)\n",
        "if hasattr(target_ds.trainloader.dataset, 'classes'):\n",
        "    classes = target_ds.trainloader.dataset.classes\n",
        "elif hasattr(target_ds.trainloader.dataset, 'labels'):\n",
        "    classes = target_ds.trainloader.dataset.labels\n",
        "else:\n",
        "    classes = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXHYSTJLTFQs"
      },
      "source": [
        "## Target Model loading or training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyqYH3wrfQcw"
      },
      "source": [
        "from hoda_training import model_class\n",
        "from hoda_utils import get_model, get_histogram\n",
        "trg_model = get_model(target_dataset=target_model_dataset,model_name=target_model_arch,pretrained=False)\n",
        "print(\"target model training inf-> target_dataset:\",target_model_dataset, \"- target_model:\", target_model_arch, \"- num_classes:\", len(classes))\n",
        "my_model = model_class(trg_model,num_classes=len(classes),classes=classes,device=device)\n",
        "loader_list = [target_ds.validloader]\n",
        "if max_hardness_degree == 99:\n",
        "  target_model_validset_loader_name = 'hoda_' + target_model_arch + '_' + target_model_dataset + '_valid_info'\n",
        "elif max_hardness_degree == 10:\n",
        "  target_model_validset_loader_name = 'hoda_' + target_model_arch + '_' + target_model_dataset + '_valid_info_' + str(max_hardness_degree)\n",
        "loader_name = [target_model_validset_loader_name]\n",
        "target_model_file_name = 'hoda_' + target_model_arch + '_' + target_model_dataset + '_model'\n",
        "if target_model_training == 'train':\n",
        "  # Loader name for each loader in loader_list is mandatory.\n",
        "  # First loader in loader_list must be validation data with the same number of classes with trainloader\n",
        "  trg_model, inf = my_model.train(target_ds.trainloader,loader_list=loader_list,loader_name=loader_name,lr=0.1,epoch=100,save_model=True,save_model_each_epoch=True,file_name=target_model_file_name,return_learning_index=True)\n",
        "trg_model,lab,pred = my_model.evaluate(target_ds.validloader,file_name=target_model_file_name,verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqQmuNjLVuew"
      },
      "source": [
        "## The hardness degree of normal samples\n",
        "\n",
        "normal samples (target model validation set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN6AxqoUWN7Q"
      },
      "source": [
        "if max_hardness_degree == 99:\n",
        "  epoch_range = np.arange(100)\n",
        "elif max_hardness_degree == 10: \n",
        "  epoch_range = np.array([0,9,19,29,39,49,59,69,79,89,99])\n",
        "else:\n",
        "  print(\"Max hardness degree\",max_hardness_degree,\"is not supported!!!\")\n",
        "#IF the hardness degree of normal samples exists, it is loaded; otherwise, the hardness degree of normal samples is calculated. \n",
        "trg_model,valid_inf = my_model.epoch_range_get_learning_index(target_ds.validloader,loader_name= target_model_validset_loader_name ,file_name=target_model_file_name,epoch_range = epoch_range)\n",
        "np.set_printoptions(linewidth=1000)\n",
        "print(get_histogram(max_hardness_degree - valid_inf.learning_index,max_hardness_degree,verbose=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpEmtDzXySuE"
      },
      "source": [
        "## The accuracy of classifiers on samples in each range of hardness degrees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO3HNVwGy3UH"
      },
      "source": [
        "from hoda_utils import draw_plot_of_trust\n",
        "if max_hardness_degree == 99:\n",
        "  _,_ = draw_plot_of_trust(valid_inf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z--WJYP9WRFF"
      },
      "source": [
        "# Model Extraction Attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWUioA9-nbn9"
      },
      "source": [
        "## Model extraction attack configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXGn5kTUnmJB"
      },
      "source": [
        "# Attack confifuration\n",
        "attack_name = 'k.net cifarx'    # valid attacks: k.net cifarx, k.net tin, jbda, and jbrand\n",
        "budget = 50000\n",
        "target_model_output = 'prob'    # prob or label\n",
        "\n",
        "#Train surrogate classifeir\n",
        "#There is no need to train a surrogate classifier to evaluate HODA. You can use 'load' option to skip the surrogate classifier training phase.\n",
        "surr_model_training = 'load' #train: train a new target model - load: load existing target model or skip the surrogate classifier training phase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4RIfUkS9sEu"
      },
      "source": [
        "## Surrogate model dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoPhMHJIHaE0"
      },
      "source": [
        "from hoda_extraction_attacks import knockoff_attack, jb_based_attacks\n",
        "if 'k.net' in attack_name:\n",
        "  if 'cifarx' in attack_name:\n",
        "    if target_model_dataset is 'cifar10':\n",
        "      attack_dataset_name = 'cifar100'\n",
        "      print(attack_name)\n",
        "    elif target_model_dataset is 'cifar100':\n",
        "      attack_dataset_name = 'cifar10'\n",
        "      print(attack_name)\n",
        "  elif 'tin' in attack_name:\n",
        "    attack_dataset_name = 'tinyimagenet'\n",
        "    print(attack_name)\n",
        "  else:\n",
        "    assert False, 'attack ' + attack_name + 'is not supported'\n",
        "\n",
        "  attack_ds = dataset_class(attack_dataset_name, train_bs = 100,local_directory= local_dir, dataset_drive_directory= ds_dir,valid = True)\n",
        "  bba = knockoff_attack(trg_model,budget=budget,num_classes=len(classes),classes=classes,device=device)\n",
        "  surr_model_trainset = bba.select_rand_data(attack_ds,target_model_dataset,attack_dataset_name,adv_access=target_model_output,imagenet_flag=False)\n",
        "  if max_hardness_degree == 99:\n",
        "    surr_model_ds_loader_name = 'hoda_' + target_model_arch + '_trg_ds_' + target_model_dataset +'_'+ attack_dataset_name +'_att_knockoff_'+str(budget) + '_' + target_model_output\n",
        "  elif max_hardness_degree == 10:\n",
        "    surr_model_ds_loader_name = 'hoda_' + target_model_arch + '_trg_ds_' + target_model_dataset +'_'+ attack_dataset_name +'_att_knockoff_'+str(budget) + '_' + target_model_output + '_' + str(max_hardness_degree)\n",
        "  surr_model_validloader = target_ds.validloader  # validation set of surrogate model is the validation set of target model dataset\n",
        "  surr_model_file_name = 'hoda_attack_' + target_model_arch + '_' + target_model_dataset + '_'+attack_dataset_name+'_knockoff_'+target_model_output+'_final_model'\n",
        "\n",
        "elif attack_name == 'jbda' or attack_name == 'jbrand':\n",
        "  if attack_name == 'jbda':\n",
        "    attack_type = 'jbda'; eps = 0.1\n",
        "  elif attack_name == 'jbrand':\n",
        "    attack_type = 'jbrand3'; eps = 64/255\n",
        "  else:\n",
        "    assert False, 'attack ' + attack_name + 'is not supported'\n",
        "\n",
        "  aux_train_epochs = 20\n",
        "  if target_model_dataset == 'cifar10':\n",
        "    num_seed_data = 500\n",
        "  elif target_model_dataset == 'cifar100':\n",
        "    num_seed_data = 1000\n",
        "\n",
        "  bba = jb_based_attacks(trg_model,target_ds,aux_classifier_name = target_model_arch,trg_ds_name=target_model_dataset, budget=budget,num_classes=len(classes),classes=classes,device=device)\n",
        "  surr_model_trainset = bba.jb_attack(attack_type=attack_type,eps=eps,aux_train_epochs=aux_train_epochs,num_seed_data = num_seed_data,adv_access=target_model_output) # HODA has been implemented by kapa=2000\n",
        "  if max_hardness_degree == 99:\n",
        "    surr_model_ds_loader_name = 'hoda_' + target_model_arch + '_trg_ds_' + target_model_dataset + '_att_'+attack_type+'_'+str(budget)+ '_' + target_model_output\n",
        "  elif max_hardness_degree == 10:\n",
        "    surr_model_ds_loader_name = 'hoda_' + target_model_arch + '_trg_ds_' + target_model_dataset + '_att_'+attack_type+'_'+str(budget)+ '_' + target_model_output + '_' + str(max_hardness_degree)\n",
        "  surr_model_validloader = bba.loader_list[0] # validation set of surrogate model is the validation set of target model dataset except seed samples\n",
        "  surr_model_file_name = 'hoda_attack_' + target_model_arch + '_' + target_model_dataset + '_'+attack_type+ '_'+target_model_output+'_final_model'\n",
        "\n",
        "else:\n",
        "  assert False, 'attack ' + attack_name + 'is not supported'\n",
        "\n",
        "surr_model_trainloader = torch.utils.data.DataLoader(surr_model_trainset, batch_size=128, shuffle=False)\n",
        "print(\"len surrgate model training set:\",len(surr_model_trainset),\", and budeget:\",budget)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rToUKDLVW-Y2"
      },
      "source": [
        "## The hardness degree histogram of attack samples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znhUOjMqBryb"
      },
      "source": [
        "#IF the hardness degree of attack samples exists, it is loaded; otherwise, the hardness degree of attack samples is calculated. \n",
        "trg_model,attack_inf = my_model.epoch_range_get_learning_index(surr_model_trainloader,loader_name= surr_model_ds_loader_name,file_name=target_model_file_name,soft_label=True,epoch_range = epoch_range )\n",
        "print(get_histogram(max_hardness_degree - attack_inf.learning_index,max_hardness_degree,verbose=1))\n",
        "print(attack_inf.pred_hist.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrzHRNgyAlB6"
      },
      "source": [
        "## Surrogate model loading or training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlQV6_HcAtj5"
      },
      "source": [
        "surr_model = get_model(target_dataset=target_model_dataset,model_name=target_model_arch,pretrained=False)\n",
        "print(\"surrogate model training inf: target_dataset:\",target_model_dataset,\"- attack:\", attack_name ,\"- target_model:\", target_model_arch, \"- num_classes:\", len(classes))\n",
        "attack_model = model_class(surr_model,num_classes=len(classes),classes=classes)\n",
        "loader_list = [surr_model_validloader]\n",
        "loader_name = ['dummy_name']\n",
        "if surr_model_training == 'train':\n",
        "  surr_model, inf = attack_model.train(surr_model_trainloader,loader_list=loader_list,loader_name=loader_name,lr=0.1,epoch=100,save_model=True,save_model_each_epoch=False,file_name=surr_model_file_name,return_learning_index=False,soft_label=True)\n",
        "#If there is no surrogate model, the next command evaluates a randomly initialized surrogate model.\n",
        "surr_model,lab_att,pred_att = attack_model.evaluate(surr_model_validloader,file_name=surr_model_file_name,verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atUkZTfvEEh6"
      },
      "source": [
        "# Hardness-Oriented Detection Approach (HODA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYgtSEWngCOb"
      },
      "source": [
        "## HODA config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oukT9qA0gBWS"
      },
      "source": [
        "fraction_of_validation_set_samples_in_S_hoda = 0.4   # 40% of normal samples for S_user (simulating benign users) and 60% of them for S_hoda (determining normal histogram H_n and calculating delta)\n",
        "pn_list = [0,25,50,75,90]                            # Pn indicates the percentage of normal samples in the adversary's sample sequence. Pn = 0 means there are no normal samples in the adversary's sample sequence.\n",
        "num_s_list = [50,100,500,1000,2000,4000]             # num_s indicates the length of sample sequences.\n",
        "\n",
        "if target_model_dataset is 'cifar10':\n",
        "  delta_list = {'50':0.29069,'100':0.15459,'500':0.03075, '1000':0.01512, '2000':0.00548,'4000':0.00314}  \n",
        "elif target_model_dataset is 'cifar100':\n",
        "  delta_list = {'50':0.71617,'100':0.34904,'500':0.04889, '1000':0.02410, '2000':0.01178,'4000':0.00631}  \n",
        "else:\n",
        "  print(\"unknown target model!!!\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7I_Rw2xTdxi"
      },
      "source": [
        "## Create normal histogram and calculate delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiuHtFfVENIf"
      },
      "source": [
        "from scipy.spatial.distance import correlation\n",
        "import os\n",
        "\n",
        "\n",
        "def create_S_hoda_and_S_user(learn_inf,max_hardness_degree,hoda_frac=0.4):\n",
        "  hardness_degree_of_normal_samples = max_hardness_degree - learn_inf\n",
        "  number_of_normal_samples = len(learn_inf)\n",
        "  fname = 'HODA/split_shoda_suser_rand_index_'+target_model_dataset+'.npy'\n",
        "  if os.path.isfile(fname): \n",
        "    shuffle_index = np.load(fname)\n",
        "    print(fname,'has been loaded.')\n",
        "  else:\n",
        "    shuffle_index = np.arange(number_of_normal_samples)\n",
        "    np.random.shuffle(shuffle_index)\n",
        "    np.save(fname,shuffle_index)\n",
        "    print(fname,'has been saved.')\n",
        "  s_hoda = hardness_degree_of_normal_samples[shuffle_index[:int(number_of_normal_samples * hoda_frac)]]\n",
        "  s_user = hardness_degree_of_normal_samples[shuffle_index[int(number_of_normal_samples * hoda_frac):]]\n",
        "  print(\"num samples S_HODA:\",len(s_hoda),\"num samples S_user:\",len(s_user))\n",
        "  return s_hoda, s_user\n",
        "\n",
        "def create_normal_histogram_and_calc_delta(s_hoda,num_s,max_hardness_degree,num_sim_seq=40000,delta_list=None):\n",
        "  fname = 'HODA/normal_histogram_'+target_model_dataset+'_num_s_'+str(num_s)+'.npy'\n",
        "  if os.path.isfile(fname) and delta_list[str(num_s)] > 0:\n",
        "    normal_hist = np.load(fname)\n",
        "    delta = delta_list[str(num_s)]\n",
        "    print(fname,' and delta has been loaded.')\n",
        "    return normal_hist, delta\n",
        "\n",
        "  list_hist = []\n",
        "  normal_hist = np.zeros(max_hardness_degree+1)\n",
        "  for i in range(num_sim_seq):\n",
        "    simulated_sample_seq = np.random.choice(s_hoda,size=num_s,replace=False)\n",
        "    hist = get_histogram(simulated_sample_seq,max_hardness_degree)\n",
        "    list_hist.append(hist)\n",
        "    normal_hist += hist\n",
        "  normal_hist = normal_hist / num_sim_seq\n",
        "  dist_list = []\n",
        "  for hist in list_hist:\n",
        "    dis = correlation(normal_hist,hist)\n",
        "    dist_list.append(dis)\n",
        "  a = plt.hist(dist_list)\n",
        "  print(a)\n",
        "  plt.show()\n",
        "  delta = np.max(dist_list)\n",
        "  np.save(fname,normal_hist)\n",
        "  print(fname,'has been saved.')\n",
        "  return normal_hist, delta\n",
        "  \n",
        "\n",
        "S_hoda, S_user = create_S_hoda_and_S_user(valid_inf.learning_index,hoda_frac=fraction_of_validation_set_samples_in_S_hoda,max_hardness_degree = max_hardness_degree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdclIY3YTjMK"
      },
      "source": [
        "## False Positive Rate and detection rate of HODA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMMe5AoV7--v"
      },
      "source": [
        "num_of_benign_users = 10000\n",
        "num_of_adversaries = 10000\n",
        "verbose = 0\n",
        "def HODA(sample_set,normal_hist,delta,num_s=None,num_simulated_user=None,max_hardness_degree=None,percentage_of_normal_samples = 0, normal_sample_set = None, verbose = 0):\n",
        "  detected_adv_sample_seq = 0\n",
        "  dist_list = []\n",
        "  if percentage_of_normal_samples > 0 and normal_sample_set is not None:\n",
        "    num_sample_in_seq = int(np.ceil(num_s * (1 - (percentage_of_normal_samples/100)))) \n",
        "    num_normal_sample_in_adaptive_attack = num_s - num_sample_in_seq\n",
        "  else:\n",
        "    num_sample_in_seq = num_s\n",
        "  for i in range(num_simulated_user):\n",
        "    sim_user_sample_seq = np.random.choice(sample_set,size=num_sample_in_seq,replace=False)\n",
        "    if percentage_of_normal_samples > 0 and normal_sample_set is not None:\n",
        "      rep_flag = False\n",
        "      if len(normal_sample_set) < num_normal_sample_in_adaptive_attack:\n",
        "        rep_flag = True\n",
        "      normal_sample_seq = np.random.choice(normal_sample_set,size=num_normal_sample_in_adaptive_attack,replace=rep_flag)\n",
        "      sim_user_sample_seq = np.concatenate((sim_user_sample_seq,normal_sample_seq),axis=None)\n",
        "    user_hist = get_histogram(sim_user_sample_seq,max_hardness_degree)\n",
        "    dis = correlation(normal_hist,user_hist)\n",
        "    dist_list.append(dis)\n",
        "    if verbose == 1:\n",
        "      plt.bar(np.arange(max_hardness_degree+1),user_hist)\n",
        "      plt.show()\n",
        "      print(dis)\n",
        "    if dis > delta:\n",
        "      detected_adv_sample_seq += 1\n",
        "  return detected_adv_sample_seq, dist_list\n",
        "\n",
        "res_arr = np.zeros((len(num_s_list),len(pn_list)))\n",
        "res_arr_i = 0\n",
        "\n",
        "for num_s in num_s_list:\n",
        "  print('**************************************************************************')\n",
        "  print('************************   Num_s = '+ str(num_s) +'  ***********************************')\n",
        "  print('**************************************************************************')\n",
        "  print()\n",
        "  print(\"******************* HODA Initialization *****************\")\n",
        "  normal_histogram, delta = create_normal_histogram_and_calc_delta(S_hoda,num_s=num_s,max_hardness_degree=max_hardness_degree,delta_list=delta_list)\n",
        "  print(\"Dleta =\",delta)\n",
        "  plt.bar(np.arange(max_hardness_degree+1),normal_histogram)\n",
        "  plt.title(\"Normal Histogram (Hn)\")\n",
        "  plt.show()\n",
        "\n",
        "  print(\"*************** HODA Evaluation ***************************\")\n",
        "  print()\n",
        "  num_of_detected_adversary, benign_user_dist_list = HODA(S_user,normal_histogram,delta,num_s=num_s,num_simulated_user=num_of_benign_users,max_hardness_degree=max_hardness_degree, verbose = verbose)\n",
        "  end = time.time()\n",
        "  FPR = np.round(num_of_detected_adversary * 100 / num_of_benign_users,2)\n",
        "  dr_list = []\n",
        "  res_arr_j = 0\n",
        "  for p in pn_list:\n",
        "    print()\n",
        "    print(\"           ********** Pn = \"+str(p)+\" **********\")\n",
        "    print()\n",
        "    num_of_detected_adversary, adv_dist_list = HODA(max_hardness_degree - attack_inf.learning_index,normal_histogram,delta,num_s=num_s,num_simulated_user=num_of_adversaries,max_hardness_degree=max_hardness_degree,percentage_of_normal_samples = p, normal_sample_set = S_user[:1000], verbose = verbose)\n",
        "    detection_rate = np.round(num_of_detected_adversary * 100 / num_of_adversaries,2)\n",
        "    dr_list.append(detection_rate)\n",
        "    plt.hist(benign_user_dist_list,label='Benign User')\n",
        "    plt.hist(adv_dist_list,label='Adversary')\n",
        "    plt.xlabel('Pearson Distance')\n",
        "    plt.title(\"Pearson Distance Histogram!\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(attack_name,\": delta =\", delta ,\"FPR =\", FPR ,\",Detection Rate =\", detection_rate)\n",
        "    print()\n",
        "    res_arr[res_arr_i,res_arr_j] = detection_rate\n",
        "    res_arr_j += 1\n",
        "  res_arr_i += 1\n",
        "\n",
        "print(\"******************* Final Result *****************\")\n",
        "for i in range(len(pn_list)):\n",
        "  plt.plot(num_s_list,res_arr[:,i],label=\"pn = \"+str(pn_list[i]))\n",
        "plt.legend()\n",
        "plt.title('num_s vs. Pn vs. Detection Rate')\n",
        "plt.xlabel(\"num_s\")\n",
        "plt.ylabel(\"Detection Rate\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
